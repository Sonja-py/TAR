# -*- coding: utf-8 -*-
"""wav2vec2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sx26F1dG5ddLDVtlcsJLPkykal1V19mG
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import torch
import torchaudio

from google.colab import drive
drive.mount('/content/drive')

bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_100H
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = bundle.get_model().to(device)

def load_l2arctic():

    # Returns 2D array: speaker, audio
    def load_audio():
      '''
      Reducing for only for one speaker
      audio = [0]*17
      for speaker in range(17):
        print(speaker)
      '''
      audio = [[0]*227]*3
      speakers = [0,1,2]

      files = os.listdir('drive/My Drive/l2arctic_data/l2arctic_recordings/0')
      files.sort()

      for speaker in speakers:
  #      audio = [0]*1131
  #      individual_audio = [0]*1131
        for i in range(0, len(files), 5):
          waveform, sample_rate = torchaudio.load('drive/My Drive/l2arctic_data/l2arctic_recordings/'+str(speaker)+'/'+files[i])
            
          # Match sampling rate with model
          if sample_rate != bundle.sample_rate:
            waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)

          audio[speaker][i//5] = waveform
#        individual_audio[i] = waveform
#      audio[speaker] = individual_audio
      return audio
    
    # Returns array of tensors for each transcript
    def load_transcript():
#      transcript_sentences = [0]*1131
      transcript_words = [0]*1131
      arctic_f = open('drive/My Drive/l2arctic_data/arctic_transcript.txt', 'r')
      i = 0
      for line in arctic_f:
        line = line.strip().upper()
#        transcript_sentences[i] = line
        transcript_words[i] = line.split()
        i+=1
      arctic_f.close()
      return transcript_words

    print('loading audio...')
    audio = load_audio()
    print('loading transcript...')
    transcript = load_transcript()

    # Separate into training/testing data sets
    # Training will not be used on wav2vec2
    print('partitioning...')
    test_audio = audio
#    test_audio = [0]*227
    test_transcript = [0]*227
    test_idx = 0
    for i in range(len(transcript)):
      if (i%5 == 0):    # 80:20 train:test split
#        test_audio[test_idx] = audio[i]
        test_transcript[test_idx] = transcript[i]
        test_idx+=1

    return test_audio, test_transcript

audio, transcripts = load_l2arctic()

# Load Librispeech corpus
test_dataset_full = torchaudio.datasets.LIBRISPEECH("./", url="test-clean", download=True)

# Reduce the amount of data so we can actually run it
audio = [0]*(2620//10)
transcripts = [0]*(2620//10)
idx=0
i=0
for item in test_dataset_full:
  if i%10==0:
    audio[idx] = item[0]
    transcripts[idx] = item[2]
    idx+=1
  i+=1

def wer(r, h):

  # costs will holds the costs, as in the Levenshtein distance algorithm
  costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]
  # backtrace will hold the operations we've done.
  # so we could later backtrace, like the WER algorithm requires us to.
  backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]
 
  OP_OK = 0
  OP_SUB = 1
  OP_INS = 2
  OP_DEL = 3
  DEL_PENALTY = 1
  INS_PENALTY = 1
  SUB_PENALTY = 1
    
  # First column represents the case where we achieve zero
  # hypothesis words by deleting all reference words.
  for i in range(1, len(r)+1):
      costs[i][0] = DEL_PENALTY*i
      backtrace[i][0] = OP_DEL
  
  # First row represents the case where we achieve the hypothesis
  # by inserting all hypothesis words into a zero-length reference.
  for j in range(1, len(h) + 1):
      costs[0][j] = INS_PENALTY * j
      backtrace[0][j] = OP_INS
    
  # computation
  for i in range(1, len(r)+1):
      for j in range(1, len(h)+1):
          if r[i-1] == h[j-1]:
              costs[i][j] = costs[i-1][j-1]
              backtrace[i][j] = OP_OK
          else:
              substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1
              insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1
              deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1
                
              costs[i][j] = min(substitutionCost, insertionCost, deletionCost)
              if costs[i][j] == substitutionCost:
                  backtrace[i][j] = OP_SUB
              elif costs[i][j] == insertionCost:
                  backtrace[i][j] = OP_INS
              else:
                  backtrace[i][j] = OP_DEL
                 
  # back trace though the best route:
  i = len(r)
  j = len(h)
  numSub = 0
  numDel = 0
  numIns = 0
  numCor = 0
  while i > 0 or j > 0:
      if backtrace[i][j] == OP_OK:
          numCor += 1
          i-=1
          j-=1
      elif backtrace[i][j] == OP_SUB:
          numSub +=1
          i-=1
          j-=1
      elif backtrace[i][j] == OP_INS:
          numIns += 1
          j-=1
      elif backtrace[i][j] == OP_DEL:
          numDel += 1
          i-=1
  result = (numSub + numDel + numIns) / (float) (len(r))
  return result


class GreedyCTCDecoder(torch.nn.Module):
    def __init__(self, labels, blank=0):
        super().__init__()
        self.labels = labels
        self.blank = blank

    def forward(self, emission: torch.Tensor) -> str:
        """Given a sequence emission over labels, get the best path string
        Args:
          emission (Tensor): Logit tensors. Shape `[num_seq, num_label]`.

        Returns:
          str: The resulting transcript
        """
        indices = torch.argmax(emission, dim=-1)  # [num_seq,]
        indices = torch.unique_consecutive(indices, dim=-1)
        indices = [i for i in indices if i != self.blank]
        return "".join([self.labels[i] for i in indices])

decoder = GreedyCTCDecoder(labels=bundle.get_labels())

def run_model(waveform):
  # Extracting audio features
  with torch.inference_mode():
      features, _ = model.extract_features(waveform)

  # Feature classification
  with torch.inference_mode():
      emission, _ = model(waveform)

  hypothesis = decoder(emission[0])
  return hypothesis

'''
Librispeech, L2Arctic single speaker
'''
wer_overall = [0]*len(audio)
for i in range(len(audio)):
  waveform = audio[i]
  reference = transcripts[i].split()
  hypothesis = run_model(waveform)
  hypothesis = hypothesis.split('|')[:-1]
  wer_overall[i] = wer(reference, hypothesis)
  if i%10==0:
    print(i)

'''
L2Arctic multiple speakers
wer_overall = [0]*(len(audio)*len(audio[0]))
wer_idx = 0

for i in range(len(audio[0])):
  reference = transcripts[i]
  for speaker in range(3):
    waveform = audio[speaker][i]
    hypothesis = run_model(waveform)
    hypothesis = hypothesis.split('|')[:-1]

    wer_overall[wer_idx] = wer(reference, hypothesis)
    wer_idx+=1

  if i%10==0:
    print(i)
'''

print(wer_overall)
wer_avg = np.average(wer_overall)

print(wer_avg)

plt.figure()
plt.hist(wer_overall, bins=16)
plt.title('wav2vec WER')